[
  {
    "objectID": "space_program_clustering.html",
    "href": "space_program_clustering.html",
    "title": "How I clustered space program data",
    "section": "",
    "text": "This is a short walkthrough of how I created clustering methods for space program analysis, to group space programs by size and scope for later discussion about the larger space ecosystem. The initial analysis Cari and I did was limited to a single dataset of active satellites as administered here by the Union of Concerned Scientists, and supporting budget and GDP data from other sources. This work has already expanded in size and scope, but it’s fun to look back at where we started!"
  },
  {
    "objectID": "space_program_clustering.html#setup",
    "href": "space_program_clustering.html#setup",
    "title": "How I clustered space program data",
    "section": "Setup",
    "text": "Setup\n\nImporting packages\n\nlibrary(tidyverse) # this is a powerful library that streamlines the data process\nlibrary(readxl) # lets us read an Excel file\nlibrary(randomForest) # one possible way to do prediction models\nlibrary(factoextra) # clustering models\nlibrary(plotly) # interactive graphics using JavaScript"
  },
  {
    "objectID": "space_program_clustering.html#data",
    "href": "space_program_clustering.html#data",
    "title": "How I clustered space program data",
    "section": "Data",
    "text": "Data\n\nImporting from .xlsx\nWe already have a super-fantastic spreadsheet to pull from where we’ve stored our collected data. Let’s import it and stick with four main variables:\n\nbudget_capita_usd: The amount of space budget, in dollars, per person.\nactive_sats: The current number of satellites a country is operating or contracting in orbit today.\ninteractions: This took a bit of behind-the-scenes calculation in Python. It’s the amount of “interaction instances,” or every single cooperation a country had with another country across all active satellites.\ninteractivity: The amount of interactions per active satellite, or a way to measure how likely a country is to cooperate with others instead of go solo.\n\n\nimport &lt;- read_excel(\"C:/Users/twcro/Documents/GitHub/SpaceSilkRoad/agency_budgets_old.xlsx\",\n                 .name_repair = janitor::make_clean_names,\n                 col_types = c(\"skip\", # Organization\n                               \"text\", # Country\n                               \"skip\", # ISO_A3\n                               \"text\", # Region\n                               \"skip\", # Annual Budget, self-est.\n                               \"skip\", # EuroConsult est., military+civil (2021)\n                               \"skip\", # Space in Africa est. (2020)\n                               \"skip\", # GDP\n                               \"skip\", # GDP/capita\n                               \"numeric\", # Budget/GDP %\n                               \"numeric\", # Budget/capita (USD)\n                               \"skip\", # Year of data\n                               \"numeric\", # Active sats.\n                               \"numeric\", # Interactions\n                               \"skip\", # Soviet former\n                               \"skip\", # ESA member\n                               \"skip\", # Arabsat investor\n                               \"skip\", # Source(s)\n                               \"skip\" # Comments\n                               )\n                 ) %&gt;%\n  na.omit() %&gt;%\n  remove_rownames %&gt;% \n  column_to_rownames(var=\"country\") %&gt;%\n  \n  # This is where I calculate \"interactivity\", or the amount of interactions\n  # per active satellites.\n  mutate(\n    interactivity = case_when(\n      active_sats != 0 ~ interactions / active_sats, \n      TRUE ~ 0\n    )\n  )\n\n# I'm removing \"region\" for our modelling df because it's not going to be used \n# in the numerical analysis\ndf &lt;- import %&gt;%\n  select(-region)\n\n\n\nAs you can see, this was long before I had learned tidyverse.\nAnd here are the first few lines of what we’re dealing with!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbudget_gdp_percent\nbudget_capita_usd\nactive_sats\ninteractions\ninteractivity\n\n\n\n\nALCE\n0.003%\n0.28\n0\n0\n0.00\n\n\nAlgeria\n0.061%\n2.26\n5\n15\n3.00\n\n\nAngola\n0.047%\n0.93\n0\n0\n0.00\n\n\nArgentina\n0.025%\n2.66\n39\n49\n1.26\n\n\nAustralia\n0.021%\n12.61\n14\n29\n2.07\n\n\nAustria\n0.018%\n9.71\n4\n8\n2.00\n\n\n\n\n\n\n\n\n\n\n\nThe PCA later shows that budget as a percent of GDP has nearly no importance, so eventually I will remove it as a feature."
  },
  {
    "objectID": "space_program_clustering.html#preprocessing",
    "href": "space_program_clustering.html#preprocessing",
    "title": "How I clustered space program data",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMulticollinearity and feature importance\nWhat is multicollinearity? It’s the idea that more than one of our variables correlate in a way that it’s hard to separate what influence comes from where.\n\n\n\nVisual description of multicollinearity\n\n\nFor instance, if both “active satellites” and “budget” seem to make the same direction of change, how do we tell if the influence is coming from one over the other?\nWhat is feature importance? It’s the degree to which a single variable seems to influence the overall data. (The math behind it is related to predictive machine learning.)\nI’m measuring both of these to determine if our initial dataset is sound.\n\nStandardize and normalize\nBefore I get to multicollinearity and feature importance, I need to examine the data structure. I’m going to scale the numeric variables from \\((-1,1)\\) to put them on equal footing. Then, I’ll take a look at how the data is distributed.\n\ndf.scale0 &lt;- as.data.frame(scale(df))\n\ndf.scale0 %&gt;% head() %&gt;% knitr::kable(caption = \"Scaled from -1 to 1\", digits = 2)\n\n\nScaled from -1 to 1\n\n\n\n\n\n\n\n\n\n\n\nbudget_gdp_percent\nbudget_capita_usd\nactive_sats\ninteractions\ninteractivity\n\n\n\n\nALCE\n-0.76\n-0.34\n-0.23\n-0.35\n-1.30\n\n\nAlgeria\n0.44\n-0.29\n-0.22\n-0.31\n1.14\n\n\nAngola\n0.16\n-0.33\n-0.23\n-0.35\n-1.30\n\n\nArgentina\n-0.31\n-0.28\n-0.14\n-0.21\n-0.28\n\n\nAustralia\n-0.39\n-0.02\n-0.20\n-0.27\n0.38\n\n\nAustria\n-0.45\n-0.10\n-0.22\n-0.32\n0.32\n\n\n\n\ndist0 &lt;- Hmisc::hist.data.frame(df.scale0)\n\n\n\n\n\n\nBehind the scenes, I did some skewness and kurtosis calculations that confirm what we can visually interpret below.\nOkay, so yikes. Look how the features are “right skewed,” meaning that a few huge outliers are present in our dataset (cough USA cough).\nIn the case of unsupervised machine learning, that might not prove to be an issue! But in the case of visualizing these things as human, these really large outliers are going to be distracting in later analysis.\nLuckily, I have a solution that doesn’t involve removing outliers. Why don’t we want to cut out outliers? Outliers, though “distracting,” are still an important part of the dataset! As we eventually make models, it’s important to remember that even the smallest and biggest programs are still parts of the ecosystem. Every model we will make is relative, and ignoring players is a certain way to draw bad conclusions.\nInstead, we can treat our data logarithmically, by applying \\({\\log (x)}\\) to columns with high skewness. This is called “normalization,” or making things more…normal. We want a nice, normal hill.\nSo, why is removing outliers not ideal, but logging okay? It’s because the relative structure and integrity of the data remains intact.\n\ndf.norm1 &lt;- df %&gt;%\n  mutate_all(~log(. + 1))\n\ndf.scale1 &lt;- as.data.frame(scale(df.norm1))\n\ndist1 &lt;- Hmisc::hist.data.frame(df.norm1)\n\n\n\n\nNice. Everything looks a lot less visually distorted when logged, so I’ll stick with df.norm1 moving forward. (Note that our goal isn’t to achieve a normal distribution here: our data is neither normal nor log-normal, and we’re not trying to assume normality.)\n\n\nFind multicollinearity\nNow back to the multicollinearity I mentioned before. Multicollinearity isn’t necessarily a bad thing when it comes to unsupervised machine learning. In fact, in some models, it can be very informative. In general, though, it indicates the degree to which a dataset is redundant.\nIn this correlation matrix, a \\(1.00\\) indicates that there is perfect correlation. (Something will always be a \\(1.00\\) with itself.) A \\(0\\) indicates no correlation at all, and a negative number indicates the opposite direction of correlation.\n\ncor_matrix &lt;- cor(df.norm1)\ncorrplot::corrplot(cor_matrix, method = \"number\")\n\n\n\n\nI’m happy with this. The fact that some variables have a relationship hovering around 0.2 or 0.3 tells me that this dataset is definitely worth exploring."
  },
  {
    "objectID": "space_program_clustering.html#principal-component-analysis",
    "href": "space_program_clustering.html#principal-component-analysis",
    "title": "How I clustered space program data",
    "section": "Principal component analysis",
    "text": "Principal component analysis\nI’m not quite yet finished with multicollinearity. I’ll come back to it, but I want to take a moment to explain principal component analysis (PCA) and why it is useful for what we are about to do.\nIn essence, PCA boils down as many features, or variables, we have into that many dimensions. A dataset with 6 variables will be given 6 dimensions after a PCA. The math is a bit mathy, but the point is that we are seeing the “essentials,” or, quite literally, the principal components, by summarizing our variables.\nThe first dimension might be dominated by variable \\(x\\), while the second dimension might show that variables \\(y\\) and \\(z\\) pull against each other. Here’s my code to build a PCA:\n\npca &lt;- prcomp(df.norm1)\n\ncomponents &lt;- pca$x %&gt;% \n  data.frame() %&gt;%\n  select(PC1,PC2,PC3) %&gt;%\n  mutate(PC2 = -PC2)\n\nexvar &lt;- summary(pca)$sdev[1:3]\n\nloadings &lt;- pca$rotation %&gt;%\n  as.data.frame() %&gt;%\n  mutate(\n    PC2 = -PC2,\n    PC4 = -PC4\n  ) %&gt;% as.matrix()\n\nscaled_loadings &lt;- sweep(loadings[, 1:length(exvar)], 2, exvar, `*`)\n\nfeatures = scaled_loadings %&gt;% row.names()\n\ncorrplot::corrplot(loadings, method = \"number\")\n\n\n\n\nBreaking our dataset down into principal components gives us an idea of how our variables are related to one another. We can see that each successive principal component has different “loadings” respective to each variable. Principal components are in order of weight. PC1 will be the most influential, while PC5 has little, if any, real say in how our data are distributed.\nFunnily enough, budget_gdp_percent is a fat goose egg until the weakest component; the PCA is telling us that budget as a percent of GDP is a terrible measure to separate space programs. I’m prepared to call a FRAUD ALERT. Just to confirm that I’m justified in removing this variable outright, let’s consider a Euclidean distance measure of the loadings to see what amounts to be a “total influence” measure:\n\n\n\n\n\n\n\n\nMagnitude\n\n\n\n\nbudget_gdp_percent\n0.000\n\n\nbudget_capita_usd\n1.261\n\n\nactive_sats\n1.898\n\n\ninteractions\n2.056\n\n\ninteractivity\n0.504\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOkay…that’s even more pathetic than I expected. This procedure is part of good data processing! The loadings gave us enough to cut out our weakest feature, making the data process a bit easier moving forward. But to draw better conclusions, let’s turn to a visual that shows the different ways our other four variables influence the distribution.\n\n\n\n\n\n\n\n\nWhy are there only 3 PCs for 4 variables?\nThere is a PC4, but the later components are weaker in importance, and we can’t visually show 4-D, so we choose to ignore it on the plot. Just because we aren’t showing it doesn’t mean we ignore it in the math later on, though.\nThis graphic is really telling! Observe how the countries are naturally falling on a general line from “big programs” to “small programs.” However, different variables have different influences.\nactive_sats, interactions, and interactivity roughly fall on a single plane, which makes sense: all three variables exclusively cover satellite activity, so in some way or another, they are going to be interrelated. Meanwhile, budget_capita_usd is nearly orthogonal (at a 90-degree angle) to this plane of satellite activity, showing us that space program budgets and activity are absolutely separate measures of space program size.\nAt face value after seeing this data, that might make sense. But classically speaking, a country with a bigger space budget should be launching more satellites, right? This PCA implies that things are actually much more nuanced."
  },
  {
    "objectID": "space_program_clustering.html#clustering",
    "href": "space_program_clustering.html#clustering",
    "title": "How I clustered space program data",
    "section": "Clustering",
    "text": "Clustering\nOkay. We’re in good shape. The PCA has confirmed that something interesting is going on. Even better, PCA takes care of all the heavy lifting for the first type of clustering we’ll try.\n\n\\(k\\)-means clustering\nDon’t get too overwhelmed: it’s a simpler concept than you think. After breaking things down into principal components, we define \\(k\\) number of clusters ourselves, and the machine learning algorithm finds that \\(k\\) amount of central means around which \\(k\\) number of groups will form.\nThink of these clusters like mini solar systems. The little data points, or planets, hovering around get pulled into the mean’s, or star’s, orbit, so that we can visually separate them!\n\n\n\nA nifty MS Paint illustration. Source: Analytics Vidhya\n\n\nWait, wait, wait. How do we decide \\(k\\)? Do we just pull a random number out of a hat? Well…I guess we could. In theory, \\(k\\)-means clustering is robust enough to maintain some extent of validity no matter how many clusters we choose (even if it might be ugly or inconclusive).\nThere’s a better way, though. I’m going to use the neat NbClust package in factoextra that uses a series of indices to tell us ideal number of clusters.\n\nfviz_nbclust(df.norm1, kmeans, method='silhouette')\n\n\n\n\nWe can see that the silhouette method tells us that 2 clusters is the ideal. Nonetheless, let’s consider the context of what we want to get out of the study. It would be nice to have a higher number of clusters that gives us a higher level analysis than “there is one strong group and one weak group.” Per the silhouette chart, \\(k\\) of 3, 4, or 6 aren’t bad choices either. For now, let’s try \\(k=4\\).\n\nplot_km &lt;- function(data, k, nboot = 2) {\n  \n  km &lt;- eclust(data, \"kmeans\", k = k, nboot = nboot)\n  \n  df_km &lt;- data %&gt;%\n    mutate(\n      k_cluster = factor(km$cluster)\n    )\n  \n  # based on the PCA, I'm going to use interactivity, active_sats and budget as axes\n  km_fig &lt;- plot_ly(df_km, \n                    x = ~budget_capita_usd, \n                    y = ~active_sats, \n                    z = ~interactivity, \n                    color = ~k_cluster,\n                    mode = 'markers',\n                    marker = list(size = 6)) %&gt;% \n    add_markers(text = rownames(df_km)) %&gt;%\n    layout(\n      plot_bgcolor = \"#e5ecf6\"\n    )\n  \n  return(km_fig)\n}\n\nplot_km(df.norm1, k = 4)\n\n\n\n\n\n\n\n\nCheck that out. We can see four machine-generated clusters. One, that includes the USA, France, and China, shows the stereotypical “big” space programs that have high levels of funding and activity.\n\n\nHierarchical clustering\nFor our final trick, let’s pull out another type of clustering. Hierarchical clustering makes a dendrogram, which is essentially a family tree of data points that shows close and distant relations. You can imagine that the “siblings” on the tree are very similar in the variables we have chosen, and the “distant cousins” aren’t.\nHow does an algorithm decide this family tree? There are two options when we hierarchically cluster.\n\n“Bottom-up” approach: the algorithm considers every country as an individual and looks for similar countries, grouping individuals into pairs then grouping pairs into larger clusters until the entire tree is connected.\n“Top-down” approach: the algorithm considers every country as one big group, and continuously finds differences to split them up until every single country is divided by those differences.\n\nFor this usage, I’m going with Ward’s minimum variance method, which is a type of “bottom-up” hierarchical clustering. If you really want to know, it uses the squared Euclidean distance between each data point to determine similarity.\n\n\nWhere \\(d_{ij}\\) is our distance measure:\n\\[\nd_{ij}=d(\\{X_i\\},\\{X_j\\})=||X_i-X_j||^2\n\\]\nSo why don’t I do some sort of test to see the “ideal” number of clusters? Unlike \\(k\\)-means clusters, where \\(k\\) directly influences the quality and integrity of the clusters, the hierarchical clustering algorithm is static, and the number of h-clusters I define is only for the purpose of visualization.\nThat’s because Ward’s method consistently computes distance. Take a look at our “family tree” below. It has a numerical y-axis labelled “Distance.” The farther the separation between two groups or countries, the farther apart they are in similarity. Look at the giant divide between the largest left and right groupings. The clustering algorithm determined that there is an extremely high-distance gap between the highest-output space programs and their younger counterparts.\nFor that reason, I find h-clustering to be more informative than \\(k\\)-means in this scenario. The family tree structure is visually powerful: for instance, take a look at the small grouping in blue of barely-funded African space programs, or the close pairing of China and India, two highly-funded but relatively young programs that are competing to break into the upper echelon of space-exploring countries.\n\n\n\n\n\n\n\nFour-tiered structure:\n\nRed: tier 1*\nGreen: tier 1\nPurple: tier 2\nTeal: tier 3\n\n\nDiscussion\nThis leaves us with four clear divisions between space programs. What exactly, can we conclude?\nFirst of all, a disclaimer: not much. There’s a tenet of “garbage in, garbage out” when it comes to unsupervised learning methods. Our initial dataset was both comprehensive and limited. The budget data and satellite data covered many countries, but we importantly lack further variables. A space program shouldn’t be defined only by “number of satellites” and “amount of budget.” The data was also flawed in the sense that budget data is entirely dependent on accessible. While some governments are forthcoming and transparent with their data, many don’t even have an official website for their space program. We relied on consultant estimates for many of the programs, which is less than reliable going forward.\nSecond of all, a point of optimism: a lot. It’s important to approach quantitative methods qualitatively. Based on existing knowledge about space programs, the results line up! Even the surprises are pretty easy to explain. Did you know that France has the biggest satellite industry outside of Russia and the United States? What we can draw from these clustering methods needs an asterisk that this was to explore methodology rather than draw definite conclusions.\nOn the general left, there is a clustering of highest-performing space programs that we generally termed tier \\(1\\). Two highlights are Ukraine and Argentina, one of which has inherited some remnants of the Soviet space program and continues to make use of it, and one of which has benefited greatly from public-private cooperation and has separated itself from its peers across the Global South.\nTo the far left, four historically powerful countries whose rocketry and satellite programs originated from the Cold War era as well as the cumulative power that is the European Space Agency differentiate themselves as \\(1^{*}\\).\nOn the general right are space programs who are, across the board, less-funded and have a lower amount of output. To the far right is tier \\(2\\). These countries shouldn’t be overlooked, a many of them focus their lesser resources into well-honed specialties, such as Kazakhstan’s thriving launch industry, or Brazil’s specialization in environmental monitoring.\nTier \\(3\\) in teal consists of up-and-comers. The distinction of being in this grouping should be a proud one, as these programs have largely arisen from comparatively the fewest resources but have still escaped Earth’s atmosphere. These countries should be watched closely as prospective partners and many of their rises will be rapid.\nAs a final visualization, take a look below at all of our countries, recontextualized with their regions and attributed “tiers” from the hierarchical clustering. Does it line up with what you might expect?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website was created with Quarto using RStudio."
  },
  {
    "objectID": "Assignment02.html",
    "href": "Assignment02.html",
    "title": "Simple ggplot exercise",
    "section": "",
    "text": "Figure 1 is a brief comparison of Tsai Ing-Wen voters and non-voters on their sentiments about Taiwanese independence.\n\n\nCode\nlibrary(haven)\nlibrary(ggplot2)\nlibrary(devtools)\nlibrary(dplyr)\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nTondulab &lt;- \"Position on unification and independence\"\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels=c(1,2,3,4,5,6,9), \n                          labels=c(\"Unification now\", \n                                   \"Status quo, unif. in future\", \n                                   \"Status quo, decide later\", \n                                   \"Status quo forever\", \n                                   \"Status quo, indep. in future\", \n                                   \"Independence now\", \n                                   \"No response\"))\n\nTEDS_Tsai &lt;- filter(TEDS_2016, votetsai==1)\nTEDS_noTsai &lt;- filter(TEDS_2016, votetsai==0)\nTEDS_NATsai &lt;- filter(TEDS_2016, is.na(votetsai))\n\n\nggplot(data=TEDS_2016, aes(x=x)) +\n  geom_bar(data=TEDS_Tsai, aes(x = Tondu), \n                 fill=\"#00A36C\") +\n  geom_bar(data=TEDS_noTsai, aes(x = Tondu), \n                 fill= \"#404080\") +\n  geom_bar(data=TEDS_NATsai, aes(x = Tondu), \n                 fill= \"#808080\") +\n  xlab(Tondulab) + ylab(\"Count\") + \n  theme(axis.text.x=element_text(angle=30,hjust=1) )\n\n\n\n\n\nFigure 1: Position on Taiwanese independence respective of Tsai vote."
  },
  {
    "objectID": "Assignment03.html",
    "href": "Assignment03.html",
    "title": "Taiwanese independence sentiments in 2016: boxplot and PCA",
    "section": "",
    "text": "The original assignment consisted of a regression nomogram of Taiwanese independence sentiments in 2016, referred to as Tondu, against named variables. I chose to remove non-respondents, which can be optionally reinstated in the code below."
  },
  {
    "objectID": "Assignment03.html#boxplot",
    "href": "Assignment03.html#boxplot",
    "title": "Taiwanese independence sentiments in 2016: boxplot and PCA",
    "section": "Boxplot",
    "text": "Boxplot\nSince this method was assigned to the students to realize it was flawed from the start, I decided to create a grouped boxplot, Figure 1, to visualize the same data. I scaled age, education, and income levels to the same metric, \\(n\\), to reflect their rough distribution across different poll answers.\n\n\nCode\n# library(regplot)\nlibrary(reshape2)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(plotly)\nlibrary(haven)\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels=c(1,2,3,4,5,6,9), \n                          labels=c(\"Unification now\", \n                                   \"Status quo, unif. in future\", \n                                   \"Status quo, decide later\", \n                                   \"Status quo forever\", \n                                   \"Status quo, indep. in future\", \n                                   \"Independence now\", \n                                   \"No response\"))\n\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\n\n# lm.fit=lm(Tondu~age+edu+income, data=TEDS_subs)\n# summary(lm.fit)\n# regplot(lm.fit)\n\n# thank you plotly for teaching me\n#  -- learning how to use '%&gt;%' from dplyr here\n# ditching the linear regression per the assignment\n# i think a boxplot model is much prettier for Tondu data\n# leaving linnmodel here for posterity's sake\n# _____________________________________________\n# lm_model1 &lt;- linear_reg() %&gt;% \n#   set_engine('lm') %&gt;% \n#   set_mode('regression') %&gt;%\n#   fit(Tondu ~ age, data = TEDS_subs) \n# \n# y &lt;- TEDS_subs$Tondu\n# x1 &lt;-TEDS_subs$age\n# \n# x1_range &lt;- seq(min(x1), max(x1), length.out = 100)\n# x1_range &lt;- matrix(x1_range, nrow=100, ncol=1)\n# x1df &lt;- data.frame(x1_range)\n# colnames(x1df) &lt;- c('age')\n# \n# y1df &lt;- lm_model1 %&gt;% predict(x1df)\n# \n# colnames(y1df) &lt;- c('Tondu')\n# xy1 &lt;- data.frame(x1df, y1df) \n# \n# fig &lt;- plot_ly(TEDS_subs, x = ~Tondu, y = ~age, type = 'scatter', alpha = 0.65, mode = 'markers', name = 'Age')\n# fig &lt;- fig %&gt;% add_trace(data = xy1, x = ~Tondu, y = ~age, name = 'Regression Fit', mode = 'lines', alpha = 1)\n# fig\n\n# Now we look at grouped boxplots\n# ________________________________\n\n# fig &lt;- plot_ly(TEDS_subs, x = ~Tondu, y = ~age, type = \"box\")\n# fig\n\n# At this point I realized that displaying Tondu against different variables \n# wouldn't work because the y-axes would be different (possible but clunky)\n\n# Going to scale age, income, edu on same scale from 0, 10\n# If necessary using maximum absolute method (x/max(x))\n\nTEDS_subs['age'] &lt;- apply(TEDS_subs['age'], 2, function(x) 10*x/max(x))\nTEDS_subs['edu'] &lt;- apply(TEDS_subs['edu'], 2, function(x) 2*x)\n\n# I found no other way to go about this but to jankily\n# rip apart my existing df, pull \"age\" \"edu\" and \"income\"\n# individually, then force it all back together in a\n# single long-form\n\nTEDS_age &lt;- TEDS_subs[c('age', 'Tondu')]\nTEDS_age &lt;- TEDS_age %&gt;% rename(\"n\" = \"age\") %&gt;% mutate(type=\"Age\")\n\nTEDS_edu &lt;- TEDS_subs[c('edu', 'Tondu')]\nTEDS_edu &lt;- TEDS_edu %&gt;% rename(\"n\" = \"edu\") %&gt;% mutate(type=\"Education\")\n\nTEDS_income &lt;- TEDS_subs[c('income', 'Tondu')]\nTEDS_income &lt;- TEDS_income %&gt;% rename(\"n\" = \"income\") %&gt;% mutate(type=\"Income\")\n\nTEDS_plot &lt;- rbind(TEDS_age, TEDS_edu, TEDS_income)\n\nfig &lt;- plot_ly(TEDS_plot, x = ~Tondu, y = ~n, color = ~type, type = \"box\")\nfig &lt;- fig %&gt;% layout(boxmode = \"group\")\nfig\n\n\n\n\n\nFigure 1: Tondu boxplot"
  },
  {
    "objectID": "Assignment03.html#d-visualization-of-data",
    "href": "Assignment03.html#d-visualization-of-data",
    "title": "Taiwanese independence sentiments in 2016: boxplot and PCA",
    "section": "3D visualization of data",
    "text": "3D visualization of data\nObviously, age, income level and education level are intricately related. Comparing them on one shared axis is inconclusive. A further analysis involving principal component analysis would give a much clearer picture on how each variable independently relates to the Tondu answers. Figure 2 is a 3D plot showing the responses we are observing.\n\n\nCode\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels=c(1,2,3,4,5,6,9), \n                          labels=c(\"Unification now\", \n                                   \"Status quo, unif. in future\", \n                                   \"Status quo, decide later\", \n                                   \"Status quo forever\", \n                                   \"Status quo, indep. in future\", \n                                   \"Independence now\", \n                                   \"No response\"))\n\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\nfig &lt;- plot_ly(TEDS_subs, x = ~age, y = ~income, z = ~edu, color = ~Tondu) %&gt;%\n  add_markers(size = 12)\nfig\n\n\n\n\n\nFigure 2: Tondu 3D"
  },
  {
    "objectID": "Assignment03.html#pca",
    "href": "Assignment03.html#pca",
    "title": "Taiwanese independence sentiments in 2016: boxplot and PCA",
    "section": "PCA",
    "text": "PCA\nTo accomplish the PCA, I assigned Tondu answers on their own scale, \\(t\\), where \\(t=1\\) indicates strong reunification sentiment and \\(t=5\\) indicates strong independence sentiment. Note that while these two ideas are opposed, further analysis should consider \\(t=3\\) “pro-status quo” to be strong and oppositional in its own right.\nBelow is a principal component analysis on age, education and income.\n\n\nCode\nlibrary(naivebayes)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(RColorBrewer)\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\nTEDS_PCA &lt;- TEDS_subs[c(\"Tondu\", \"age\", \"edu\", \"income\")]\nTEDS_PCA$Tondu &lt;- ifelse(TEDS_PCA$Tondu &gt; 3,\n                  TEDS_PCA$Tondu-1,\n                  TEDS_PCA$Tondu)\n\npr.mas=prcomp(TEDS_PCA, scale=TRUE)\n\nfviz_pca_biplot(pr.mas, label=\"var\", col.var=\"firebrick1\")\n\n\n\n\n\nTondu PCA"
  },
  {
    "objectID": "Assignment03.html#further-exploration-of-data",
    "href": "Assignment03.html#further-exploration-of-data",
    "title": "Taiwanese independence sentiments in 2016: boxplot and PCA",
    "section": "Further exploration of data",
    "text": "Further exploration of data\nI thought about visualizing a 3-dimensional PCA to see if categorization without factoring in Tondu arose, but found that relying on only 3 variables (age, education, income) made increasing components unnecessary: the total explained variance was already plenty high and the resulting visualization was just a slop. Technically addressing more parts of the Tondu polling data was outside the scope of the initial assignment, but perhaps I’ll readdress it with more variables another time, as well as reintroducing Tondu into the PCA on its 1:5 scale. That’s all for now.\n– 8 Mar"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Taylor Crockett",
    "section": "",
    "text": "Academic purposes:\ntaylorcrockett@utdallas.edu\nOther inquiries:\ntwc.rocket@gmail.com\n\n\n\n\n\n\n\n\n\nTwitter status (subject to change):\n\n\nI’m officially leaving Twitter. i spend entirely too much time on here. Take care everyone. I’ll be back in 15 minutes.\n\n— liddy ☆ 🎪 (@pr1ncette) August 25, 2020"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Unable to display .pdf file. Download it here instead."
  },
  {
    "objectID": "EnvironmentalRacismGreenSpace.html",
    "href": "EnvironmentalRacismGreenSpace.html",
    "title": "Environmental Racism and Green Space (2022)",
    "section": "",
    "text": "This paper was a major catalyst in me wanting to learn GIS and better my understanding of data analysis. At a future date I want to revisit this research with the tools I now have (instead of using MS Paint to draw borders in a .png file…yikes).\n\n\nUnable to display .pdf file. Download it here instead."
  },
  {
    "objectID": "LunarPhases.html",
    "href": "LunarPhases.html",
    "title": "Lunar Phases and Fatal Crashes",
    "section": "",
    "text": "A long time ago, after I nearly got hit on the way to her house, a family friend told me that people always get loony (appropriate terminology) during full moons—be careful on the road when the moon is bright. A silly wives’ tale, I told myself. Certainly, there was nothing to back it up. The legitimacy of an idea like clinical lunacy had long been debunked. But then I encountered another erratic driver on the way home, and checked Google. Sure enough, it was a full moon that day. It’s not that I’ve ever been truly been convinced, but the idea has long eaten at me, mostly because in my experience it has nearly always turned out to be true.\nA driver nearly runs me off the road? I check, and it’s a full moon that day. My friend gets slammed into at a red light? It happened on a full moon. And just a few months ago, I barely dodge the drunkest of drivers: it was a full moon. Even a handful of my most cynical friends have jumped on board the loony driver train.\nIt’s been years now, and I finally decided that I can’t let this gnaw at my brain any longer. So I did what any reasonable person would do with their first free weekend after graduating from undergrad: I collected data from the National Highway Traffic Safety Administration’s very robust Fatality Analysis Reporting System (FARS), prepared a gradient boosting tree model, and constructed a hypothesis test."
  },
  {
    "objectID": "LunarPhases.html#setting-things-up",
    "href": "LunarPhases.html#setting-things-up",
    "title": "Lunar Phases and Fatal Crashes",
    "section": "Setting things up",
    "text": "Setting things up\nSince this method was assigned to the students to realize it was flawed from the start, I decided to create a grouped boxplot, Figure 1, to visualize the same data. I scaled age, education, and income levels to the same metric, \\(n\\), to reflect their rough distribution across different poll answers.\n\n\nCode\n# library(regplot)\nlibrary(reshape2)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(plotly)\nlibrary(haven)\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels=c(1,2,3,4,5,6,9), \n                          labels=c(\"Unification now\", \n                                   \"Status quo, unif. in future\", \n                                   \"Status quo, decide later\", \n                                   \"Status quo forever\", \n                                   \"Status quo, indep. in future\", \n                                   \"Independence now\", \n                                   \"No response\"))\n\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\n\n# lm.fit=lm(Tondu~age+edu+income, data=TEDS_subs)\n# summary(lm.fit)\n# regplot(lm.fit)\n\n# thank you plotly for teaching me\n#  -- learning how to use '%&gt;%' from dplyr here\n# ditching the linear regression per the assignment\n# i think a boxplot model is much prettier for Tondu data\n# leaving linnmodel here for posterity's sake\n# _____________________________________________\n# lm_model1 &lt;- linear_reg() %&gt;% \n#   set_engine('lm') %&gt;% \n#   set_mode('regression') %&gt;%\n#   fit(Tondu ~ age, data = TEDS_subs) \n# \n# y &lt;- TEDS_subs$Tondu\n# x1 &lt;-TEDS_subs$age\n# \n# x1_range &lt;- seq(min(x1), max(x1), length.out = 100)\n# x1_range &lt;- matrix(x1_range, nrow=100, ncol=1)\n# x1df &lt;- data.frame(x1_range)\n# colnames(x1df) &lt;- c('age')\n# \n# y1df &lt;- lm_model1 %&gt;% predict(x1df)\n# \n# colnames(y1df) &lt;- c('Tondu')\n# xy1 &lt;- data.frame(x1df, y1df) \n# \n# fig &lt;- plot_ly(TEDS_subs, x = ~Tondu, y = ~age, type = 'scatter', alpha = 0.65, mode = 'markers', name = 'Age')\n# fig &lt;- fig %&gt;% add_trace(data = xy1, x = ~Tondu, y = ~age, name = 'Regression Fit', mode = 'lines', alpha = 1)\n# fig\n\n# Now we look at grouped boxplots\n# ________________________________\n\n# fig &lt;- plot_ly(TEDS_subs, x = ~Tondu, y = ~age, type = \"box\")\n# fig\n\n# At this point I realized that displaying Tondu against different variables \n# wouldn't work because the y-axes would be different (possible but clunky)\n\n# Going to scale age, income, edu on same scale from 0, 10\n# If necessary using maximum absolute method (x/max(x))\n\nTEDS_subs['age'] &lt;- apply(TEDS_subs['age'], 2, function(x) 10*x/max(x))\nTEDS_subs['edu'] &lt;- apply(TEDS_subs['edu'], 2, function(x) 2*x)\n\n# I found no other way to go about this but to jankily\n# rip apart my existing df, pull \"age\" \"edu\" and \"income\"\n# individually, then force it all back together in a\n# single long-form\n\nTEDS_age &lt;- TEDS_subs[c('age', 'Tondu')]\nTEDS_age &lt;- TEDS_age %&gt;% rename(\"n\" = \"age\") %&gt;% mutate(type=\"Age\")\n\nTEDS_edu &lt;- TEDS_subs[c('edu', 'Tondu')]\nTEDS_edu &lt;- TEDS_edu %&gt;% rename(\"n\" = \"edu\") %&gt;% mutate(type=\"Education\")\n\nTEDS_income &lt;- TEDS_subs[c('income', 'Tondu')]\nTEDS_income &lt;- TEDS_income %&gt;% rename(\"n\" = \"income\") %&gt;% mutate(type=\"Income\")\n\nTEDS_plot &lt;- rbind(TEDS_age, TEDS_edu, TEDS_income)\n\nfig &lt;- plot_ly(TEDS_plot, x = ~Tondu, y = ~n, color = ~type, type = \"box\")\nfig &lt;- fig %&gt;% layout(boxmode = \"group\")\nfig\n\n\n\n\n\nFigure 1: Tondu boxplot"
  },
  {
    "objectID": "LunarPhases.html#d-visualization-of-data",
    "href": "LunarPhases.html#d-visualization-of-data",
    "title": "Lunar Phases and Fatal Crashes",
    "section": "3D visualization of data",
    "text": "3D visualization of data\nObviously, age, income level and education level are intricately related. Comparing them on one shared axis is inconclusive. A further analysis involving principal component analysis would give a much clearer picture on how each variable independently relates to the Tondu answers. Figure 2 is a 3D plot showing the responses we are observing.\n\n\nCode\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu, levels=c(1,2,3,4,5,6,9), \n                          labels=c(\"Unification now\", \n                                   \"Status quo, unif. in future\", \n                                   \"Status quo, decide later\", \n                                   \"Status quo forever\", \n                                   \"Status quo, indep. in future\", \n                                   \"Independence now\", \n                                   \"No response\"))\n\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\nfig &lt;- plot_ly(TEDS_subs, x = ~age, y = ~income, z = ~edu, color = ~Tondu) %&gt;%\n  add_markers(size = 12)\nfig\n\n\n\n\n\nFigure 2: Tondu 3D"
  },
  {
    "objectID": "LunarPhases.html#pca",
    "href": "LunarPhases.html#pca",
    "title": "Lunar Phases and Fatal Crashes",
    "section": "PCA",
    "text": "PCA\nTo accomplish the PCA, I assigned Tondu answers on their own scale, \\(t\\), where \\(t=1\\) indicates strong reunification sentiment and \\(t=5\\) indicates strong independence sentiment. Note that while these two ideas are opposed, further analysis should consider \\(t=3\\) “pro-status quo” to be strong and oppositional in its own right.\nBelow is a principal component analysis on age, education and income.\n\n\nCode\nlibrary(naivebayes)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(RColorBrewer)\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# opt. remove non-responses before relabelling\nTEDS_2016 &lt;- filter(TEDS_2016, Tondu &lt; 8)\n\nnewvars &lt;- c(\"Tondu\", \"female\", \"DPP\", \"age\", \"income\", \"edu\", \"Taiwanese\", \"Econ_worse\")\nTEDS_subs &lt;- TEDS_2016[complete.cases(TEDS_2016[ , newvars]), ]\n\nTEDS_PCA &lt;- TEDS_subs[c(\"Tondu\", \"age\", \"edu\", \"income\")]\nTEDS_PCA$Tondu &lt;- ifelse(TEDS_PCA$Tondu &gt; 3,\n                  TEDS_PCA$Tondu-1,\n                  TEDS_PCA$Tondu)\n\npr.mas=prcomp(TEDS_PCA, scale=TRUE)\n\nfviz_pca_biplot(pr.mas, label=\"var\", col.var=\"firebrick1\")\n\n\n\n\n\nTondu PCA"
  },
  {
    "objectID": "LunarPhases.html#further-exploration-of-data",
    "href": "LunarPhases.html#further-exploration-of-data",
    "title": "Lunar Phases and Fatal Crashes",
    "section": "Further exploration of data",
    "text": "Further exploration of data\nI thought about visualizing a 3-dimensional PCA to see if categorization without factoring in Tondu arose, but found that relying on only 3 variables (age, education, income) made increasing components unnecessary: the total explained variance was already plenty high and the resulting visualization was just a slop. Technically addressing more parts of the Tondu polling data was outside the scope of the initial assignment, but perhaps I’ll readdress it with more variables another time, as well as reintroducing Tondu into the PCA on its 1:5 scale. That’s all for now.\n– 8 Mar"
  },
  {
    "objectID": "SpaceSilkRoad.html",
    "href": "SpaceSilkRoad.html",
    "title": "Space Silk Road (2021)",
    "section": "",
    "text": "A 2021 case study on “developing space powers,” which we now call “emerging space powers” as to avoid harmful connotations.\n\n\nUnable to display .pdf file. Download it here instead."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taylor Crockett",
    "section": "",
    "text": "This is the website of Taylor Crockett, to demonstrate research and projects. You can find my CV on the top menu bar.\nI am an undergraduate student at the University of Texas at Dallas (B.S. International Political Economy, grad. May 2023) and fast-tracked graduate student (M.S. Social Data Analytics and Research).\n\n\n\nResearch\n\nDeveloping space powers\nMost current research involves work about “developing space powers” (DSPs), or countries with burgeoning space programs, and how and where their development places them in regard to the space economy and international relations. This research is pending later publication.\n\n\nSpace Silk Road\nPrevious research on this topic focused on the battle of influence between China and the US: how declining US interest, especially during the Trump presidency, led smaller satellite-operating countries to seek Chinese resources under the umbrella of the Belt and Road Initiative. The paper, as was presented at the WPSA and SPSA conferences in 2022, can be found here.\n\n\n\nInterests\n\nData science\nInternational relations\nInternational political economy\nMapping and environmental research\nPolicy\n\nSpace policy\nUrban planning/policy\nEnvironmental policy\n\n\n\n\nContact me"
  },
  {
    "objectID": "assignment_02.html",
    "href": "assignment_02.html",
    "title": "EPPS 6302: Assignment 2",
    "section": "",
    "text": "Google Trends lets users decide between using “search terms” and “topics,” but what’s the difference?\n\n\nUsing search terms\n\n\n\nUsing topics\n\n\n\n\nAnimals, cars and football\n“Search terms” are single keywords, while “topics” are a group of keywords that Google curates as a better measure of true popularity. The difference in quality can be very pronounced when investigating non-distinct or very niche topics.\n\n\n\n\n\nSearch term “jaguar” compared to topics “Jaguar (Animal)”, “Jaguar (Car make)”, “Jacksonville Jaguars (Football team)”"
  },
  {
    "objectID": "assignment_03.html",
    "href": "assignment_03.html",
    "title": "EPPS 6302: Assignment 3",
    "section": "",
    "text": "This assignment aims to analyze Twitter data related to the President Biden and Xi summit in November 2021. We will explore public sentiment, popular hashtags, and user engagement through various text analysis techniques."
  },
  {
    "objectID": "assignment_03.html#introduction",
    "href": "assignment_03.html#introduction",
    "title": "EPPS 6302: Assignment 3",
    "section": "",
    "text": "This assignment aims to analyze Twitter data related to the President Biden and Xi summit in November 2021. We will explore public sentiment, popular hashtags, and user engagement through various text analysis techniques."
  },
  {
    "objectID": "assignment_03.html#data-loading-and-preparation",
    "href": "assignment_03.html#data-loading-and-preparation",
    "title": "EPPS 6302: Assignment 3",
    "section": "Data Loading and Preparation",
    "text": "Data Loading and Preparation\n\n\nCode\nlibrary(quanteda)\n\n\nWarning: package 'quanteda' was built under R version 4.2.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"xMatrix\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"mMatrix\"; definition not updated\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"pcorMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\n\nCode\nlibrary(quanteda.textmodels)\n\n\nWarning: package 'quanteda.textmodels' was built under R version 4.2.3\n\n\nCode\nlibrary(quanteda.textplots)\n\n\nWarning: package 'quanteda.textplots' was built under R version 4.2.3\n\n\nCode\nlibrary(readr)\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nCode\nlibrary(knitr)\n\n\nWarning: package 'knitr' was built under R version 4.2.3\n\n\nCode\nlibrary(tidyverse)\n\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ stringr   1.5.0\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# Load the data\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\n\nRows: 14520 Columns: 90\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# Display the data\nkable(head(summit))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nstatus_id\ncreated_at\nscreen_name\ntext\nsource\ndisplay_text_width\nreply_to_status_id\nreply_to_user_id\nreply_to_screen_name\nis_quote\nis_retweet\nfavorite_count\nretweet_count\nquote_count\nreply_count\nhashtags\nsymbols\nurls_url\nurls_t.co\nurls_expanded_url\nmedia_url\nmedia_t.co\nmedia_expanded_url\nmedia_type\next_media_url\next_media_t.co\next_media_expanded_url\next_media_type\nmentions_user_id\nmentions_screen_name\nlang\nquoted_status_id\nquoted_text\nquoted_created_at\nquoted_source\nquoted_favorite_count\nquoted_retweet_count\nquoted_user_id\nquoted_screen_name\nquoted_name\nquoted_followers_count\nquoted_friends_count\nquoted_statuses_count\nquoted_location\nquoted_description\nquoted_verified\nretweet_status_id\nretweet_text\nretweet_created_at\nretweet_source\nretweet_favorite_count\nretweet_retweet_count\nretweet_user_id\nretweet_screen_name\nretweet_name\nretweet_followers_count\nretweet_friends_count\nretweet_statuses_count\nretweet_location\nretweet_description\nretweet_verified\nplace_url\nplace_name\nplace_full_name\nplace_type\ncountry\ncountry_code\ngeo_coords\ncoords_coords\nbbox_coords\nstatus_url\nname\nlocation\ndescription\nurl\nprotected\nfollowers_count\nfriends_count\nlisted_count\nstatuses_count\nfavourites_count\naccount_created_at\nverified\nprofile_url\nprofile_expanded_url\naccount_lang\nprofile_banner_url\nprofile_background_url\nprofile_image_url\n\n\n\n\n1.375230e+18\n1.460702e+18\n2021-11-16 20:10:23\nDSJ78992721\nBreaking News: US President Biden & communist china leader, xi jinpig, pledged at a virtual summit to improve cooperation, but china offered no major breakthroughs.\nTwitter for iPhone\n144\nNA\nNA\nNA\nFALSE\nTRUE\n0\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1329215021641961472\nGundamNorthrop\nen\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.460498e+18\nBreaking News: US President Biden & communist china leader, xi jinpig, pledged at a virtual summit to improve cooperation, but china offered no major breakthroughs.\n2021-11-16 06:40:33\nTwitter Web App\n18\n7\n1.329215e+18\nGundamNorthrop\nNorthrop Gundam\n376\n519\n27968\nNA\nNA\nFALSE\nNA\nNA\nNA\nNA\nNA\nNA\n|\n|\n|||||||\nhttps://twitter.com/DSJ78992721/status/1460701806644715521\nWilson Edwards\nNYC\nUC Berkeley Econ ’04, Investment Banking, Corgis, Providing 4 my beautiful family, free speech, fighting 4 those w/o a voice, & drinking CCP Troll Tears\nhttps://t.co/QdxZ3bZuEd\nFALSE\n219\n1017\n1\n5360\n5280\n2021-03-25 23:36:23\nFALSE\nhttps://t.co/QdxZ3bZuEd\nhttp://wumao.com\nNA\nhttps://pbs.twimg.com/profile_banners/1375230026120003585/1616716661\nNA\nhttp://pbs.twimg.com/profile_images/1375230497257713664/aKjtKvRF_normal.jpg\n\n\n2.600418e+08\n1.460702e+18\n2021-11-16 20:10:17\nbradhooperarch\nhttps://t.co/rKRzwyIvcy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeter Dutton so disappointed |Twitter for iPad | 71| NA| NA|NA |FALSE |TRUE | 0| 7|NA |NA |NA |NA |theage.com.au/world/north-am… |https://t.co/rKRzwyIvcy |https://www.theage.com.au/world/north-america/my-old-friend-biden-xi-begin-superpower-summit-on-friendly-note-20211116-p599cj.html?btis |NA |NA |NA |NA |NA |NA |NA |NA |289822163 |annschof_ann |en | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA | 1.460695e+18|https://t.co/rKRzwyIvcy\nPeter Dutton so disappointed |2021-11-16 19:44:16 |Twitter for iPad | 16| 7| 2.898222e+08|annschof_ann |ann schofield✳️✳️ | 2416| 3240| 19496|Brunswick Victoria Australia |Stoic. Doubled vaxxed. Reader, writer, educator, streamer of long form drama, living on Wurundjeri land. ☕️🐀💉 |FALSE |NA |NA |NA |NA |NA |NA || || |||||||| |https://twitter.com/bradhooperarch/status/1460701781915103232 |Brad Hooper 🔴⚪️💙 |Australia |Architect and urban designer from Central Victoria. |https://t.co/gVltBbOihc |FALSE | 2279| 1911| 159| 328138| 147877|2011-03-03 02:47:37 |FALSE |https://t.co/gVltBbOihc |http://bradhooperarchitecture.com |NA |https://pbs.twimg.com/profile_banners/260041830/1609978990 |http://abs.twimg.com/images/themes/theme6/bg.gif |http://pbs.twimg.com/profile_images/1174623153663827968/B-VnzPwM_normal.jpg | | 3.004364e+09| 1.460702e+18|2021-11-16 20:10:10 |scarecrow1113 |[Recap] Biden urges ‘guardrails’ against conflict in virtual Xi summit\nhttps://t.co/ezK97cFBG4 |Twitter Web App | 105| NA| NA|NA |FALSE |TRUE | 0| 5|NA |NA |NA |NA |hongkongfp.com/2021/11/16/bid… |https://t.co/ezK97cFBG4 |https://hongkongfp.com/2021/11/16/biden-urges-guardrails-against-conflict-in-virtual-xi-summit |NA |NA |NA |NA |NA |NA |NA |NA |3071162052 |hkfp |en | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA | 1.460625e+18|[Recap] Biden urges ‘guardrails’ against conflict in virtual Xi summit\nhttps://t.co/ezK97cFBG4 |2021-11-16 15:04:45 |Buffer | 4| 5| 3.071162e+09|hkfp |Hong Kong Free Press HKFP | 376450| 20| 49620|Hong Kong |Non-profit, impartial Hong Kong news. Backed by readers, governed by an ethics code, 100% independent & no paywall. Contact: https://t.co/nMZjhaI1MT |TRUE |NA |NA |NA |NA |NA |NA || || |||||||| |https://twitter.com/scarecrow1113/status/1460701749266685952 |liuwailing |NA |NA |NA |FALSE | 160| 19| 1| 62255| 31839|2015-01-31 05:50:42 |FALSE |NA |NA |NA |NA |http://abs.twimg.com/images/themes/theme1/bg.png |http://pbs.twimg.com/profile_images/1205454452787793920/fAawRZ_7_normal.jpg | | 3.004364e+09| 1.460328e+18|2021-11-15 19:24:04 |scarecrow1113 |U.S. President Joe Biden and his Chinese counterpart Xi Jinping are expected to discuss damage control, rather than resolution of key differences, as the main focus of their video summit later today Washington time.\nhttps://t.co/iUNHwMvsAO |Twitter Web App | 140| NA| NA|NA |FALSE |TRUE | 0| 5|NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |16561457 |RadioFreeAsia |en | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA | 1.460266e+18|U.S. President Joe Biden and his Chinese counterpart Xi Jinping are expected to discuss damage control, rather than resolution of key differences, as the main focus of their video summit later today Washington time.\nhttps://t.co/iUNHwMvsAO |2021-11-15 15:16:45 |TweetDeck | 5| 5| 1.656146e+07|RadioFreeAsia |Radio Free Asia | 58042| 24| 25162|Washington, DC |Delivering reliable, uncensored news and providing an open forum for citizens in Asian countries that restrict media, free press and free speech. RT≠Endorsement |TRUE |NA |NA |NA |NA |NA |NA || || |||||||| |https://twitter.com/scarecrow1113/status/1460327759855587329 |liuwailing |NA |NA |NA |FALSE | 160| 19| 1| 62255| 31839|2015-01-31 05:50:42 |FALSE |NA |NA |NA |NA |http://abs.twimg.com/images/themes/theme1/bg.png |http://pbs.twimg.com/profile_images/1205454452787793920/fAawRZ_7_normal.jpg | | 1.361768e+18| 1.460493e+18|2021-11-16 06:22:29 |Internl_Leaks |#BREAKING Biden opens virtual summit with China’s Xi from White House\n#BreakingNews #Biden #China #Usa |Twitter for Android | 103| NA| NA|NA |FALSE |FALSE | 0| 3|NA |NA |BREAKING|BreakingNews|Biden|China|Usa |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |en | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA |NA |NA |NA |NA |NA |NA || || |||||||| |https://twitter.com/Internl_Leaks/status/1460493456330006529 |International Leaks |World |Geopolitics - Foreign Policy - International Leaks YouTube - https://t.co/tktmC4b4Cs Facebook - https://t.co/Bq4Qz4501m |NA |FALSE | 3513| 0| 51| 19150| 269|2021-02-16 20:04:26 |FALSE |NA |NA |NA |https://pbs.twimg.com/profile_banners/1361768369100181506/1631424178 |NA |http://pbs.twimg.com/profile_images/1437134165497942016/vFB2QGW4_normal.jpg | | 1.361768e+18| 1.460702e+18|2021-11-16 20:09:36 |Internl_Leaks |#BREAKING Biden opens virtual summit with China’s Xi from White House\n#BreakingNews #Biden #China #Usa |Twitter for Android | 122| NA| NA|NA |FALSE |TRUE | 0| 3|NA |NA |BREAKING|BreakingNews|Biden|China|Usa |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |NA |1361768369100181506 |Internl_Leaks |en | NA|NA |NA |NA | NA| NA| NA|NA |NA | NA| NA| NA|NA |NA |NA | 1.460493e+18|#BREAKING Biden opens virtual summit with China’s Xi from White House\n#BreakingNews #Biden #China #Usa |2021-11-16 06:22:29 |Twitter for Android | 0| 3| 1.361768e+18|Internl_Leaks |International Leaks | 3513| 0| 19150|World |Geopolitics - Foreign Policy - International Leaks YouTube - https://t.co/tktmC4b4Cs Facebook - https://t.co/Bq4Qz4501m |FALSE |NA |NA |NA |NA |NA |NA || || |||||||| |https://twitter.com/Internl_Leaks/status/1460701607910264832 |International Leaks |World |Geopolitics - Foreign Policy - International Leaks YouTube - https://t.co/tktmC4b4Cs Facebook - https://t.co/Bq4Qz4501m |NA |FALSE | 3513| 0| 51| 19150| 269|2021-02-16 20:04:26 |FALSE |NA |NA |NA |https://pbs.twimg.com/profile_banners/1361768369100181506/1631424178 |NA |http://pbs.twimg.com/profile_images/1437134165497942016/vFB2QGW4_normal.jpg |"
  },
  {
    "objectID": "assignment_06.html",
    "href": "assignment_06.html",
    "title": "EPPS 6302: Assignment 6",
    "section": "",
    "text": "I believe in Python supremacy, so here’s a simple example of one of the methods I use to scrape or download in parallel instances: the asyncio package. It’s not let me down thus far.\n\n\nCode\nimport asyncio\nimport aiohttp\nimport aiosqlite\n\nasync def download_pdf(session, url):\n    async with session.get(url) as response:\n        if response.status == 200:\n            return await response.read()\n        else:\n            raise Exception(f\"Error downloading {url}\")\n\nasync def save_pdf_to_db(db, url, data):\n    async with db.execute(\"INSERT INTO pdfs (url, content) VALUES (?, ?)\", (url, data)):\n        await db.commit()\n\nasync def main(urls):\n    async with aiohttp.ClientSession() as session:\n        # Create database and table\n        db = await aiosqlite.connect(\"pdf_database.db\")\n        await db.execute(\"CREATE TABLE IF NOT EXISTS pdfs (url TEXT, content BLOB)\")\n\n        tasks = []\n        for url in urls:\n            task = asyncio.create_task(download_pdf(session, url))\n            tasks.append(task)\n\n        pdf_contents = await asyncio.gather(*tasks)\n\n        for url, content in zip(urls, pdf_contents):\n            await save_pdf_to_db(db, url, content)\n\n        await db.close()"
  },
  {
    "objectID": "assignment_03.html#latent-semantic-analysis-lsa",
    "href": "assignment_03.html#latent-semantic-analysis-lsa",
    "title": "EPPS 6302: Assignment 3",
    "section": "Latent Semantic Analysis (LSA)",
    "text": "Latent Semantic Analysis (LSA)\nLatent Semantic Analysis helps us understand underlying themes.\n\n\nCode\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\n# Perform LSA\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\n\n# Plot LSA results\nlsa_dim1 &lt;- sum_lsa$docs[,1]\nlsa_dim2 &lt;- sum_lsa$docs[,2]\n\nplot(lsa_dim1, lsa_dim2, xlab = \"LSA Dimension 1\", ylab = \"LSA Dimension 2\", main = \"LSA of Summit Tweets\", pch = 20)"
  },
  {
    "objectID": "assignment_03.html#hashtag-analysis",
    "href": "assignment_03.html#hashtag-analysis",
    "title": "EPPS 6302: Assignment 3",
    "section": "Hashtag Analysis",
    "text": "Hashtag Analysis\nAnalyzing the most popular hashtags.\n\n\nCode\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\n# Extract top 50 hashtags\ntop_hashtags &lt;- topfeatures(tag_dfm, 25)\n\n# Convert to data frame for plotting\ndf_hashtags &lt;- data.frame(\n  hashtag = names(top_hashtags),\n  frequency = top_hashtags\n)\n\n# Plot the bar graph\nggplot(df_hashtags, aes(x = reorder(hashtag, frequency), y = frequency)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flips the axes for easier reading\n  labs(title = \"Top 25 Hashtags\", x = \"Hashtag\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "assignment_03.html#hashtag-network-analysis",
    "href": "assignment_03.html#hashtag-network-analysis",
    "title": "EPPS 6302: Assignment 3",
    "section": "Hashtag Network Analysis",
    "text": "Hashtag Network Analysis\nVisualizing how different hashtags are related.\n\n\nCode\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\n\n# Create a network plot of hashtags\ntag_fcm &lt;- fcm(dfm_select(tweet_dfm, pattern = \"#*\"))\ntoptag &lt;- names(topfeatures(dfm_select(tweet_dfm, pattern = \"#*\"), 50))\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)"
  },
  {
    "objectID": "assignment_03.html#user-mention-analysis",
    "href": "assignment_03.html#user-mention-analysis",
    "title": "EPPS 6302: Assignment 3",
    "section": "User Mention Analysis",
    "text": "User Mention Analysis\nAnalyzing the most frequently mentioned users.\n\n\nCode\n# Extract user mentions\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 10, edge_color = \"#e05248\", edge_alpha = 0.8, edge_size = 5)"
  },
  {
    "objectID": "assignment_08.html",
    "href": "assignment_08.html",
    "title": "EPPS 6302: Assignment 8",
    "section": "",
    "text": "What changed in Texas between 2010 and 2020?\n\n\nCode\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(scales)\n\n\ndata_2010 &lt;- get_acs(geography = \"place\", \n                            variables = c(population =\"B01001_001\", median_income = \"B07011_001\"),\n                            year = 2010, \n                            state = \"TX\", \n                            survey = \"acs5\") %&gt;%\n  mutate(year = 2010)\n\ndata_2020 &lt;- get_acs(geography = \"place\", \n                     variables = c(population =\"B01001_001\", median_income = \"B07011_001\"),\n                     year = 2020, \n                     state = \"TX\", \n                     survey = \"acs5\", \n                     geometry = T) %&gt;%\n  mutate(year = 2020)\n\ndata &lt;- bind_rows(data_2010, sf::st_drop_geometry(data_2020))\nchange &lt;- data %&gt;% \n  mutate(variable = paste0(variable, \"_\", year)) %&gt;%\n  select(-moe, -year) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  mutate(median_income_2010 = median_income_2010 * 1.1905,\n         pop_change = (population_2020 - population_2010) / population_2010,\n         inc_change = (median_income_2020 - median_income_2010) / median_income_2010,\n         pop_change = ifelse(pop_change == Inf, NA, pop_change),\n         inc_change = ifelse(inc_change == Inf, NA, inc_change),\n         pop_change_color = case_when(\n           pop_change &gt;= 2.5 ~ 2.49,\n           T ~ pop_change\n         ),\n         inc_change_color = case_when(\n           inc_change &gt;= 2.5 ~ 2.49,\n           T ~ inc_change\n         )) %&gt;%\n  left_join(data_2020 %&gt;% select(GEOID, geometry), by = \"GEOID\") %&gt;%\n  sf::st_as_sf()\n\n## Make vector of colors for values smaller than 0 (20 colors)\nrc1 &lt;- colorRampPalette(colors = c(\"#3c23a8\", \"#dadada\"), space = \"Lab\")(50)\n\n## Make vector of colors for values larger than 0 (180 colors)\nrc2 &lt;- colorRampPalette(colors = c(\"#dadada\", \"#ed851c\"), space = \"Lab\")(125)\n\n## Combine the two color palettes\nrampcols &lt;- c(rc1, rc2)\n\ncolorPalette &lt;- colorNumeric(\n    palette = rampcols, \n    domain = c(-1, 2.5),\n    na.color = \"#ffffff\"\n)\n\n## Make vector of colors for values smaller than 0 (20 colors)\nrc1.2 &lt;- colorRampPalette(colors = c(\"#b32315\", \"#dadada\"), space = \"Lab\")(100)\n\n## Make vector of colors for values larger than 0 (180 colors)\nrc2.2 &lt;- colorRampPalette(colors = c(\"#dadada\", \"#02c72d\"), space = \"Lab\")(200)\n\n## Combine the two color palettes\nrampcols.2 &lt;- c(rc1.2, rc2.2)\n\ncolorPalette2 &lt;- colorNumeric(\n    palette = rampcols.2, \n    domain = c(-1, 2),\n    na.color = \"#ffffff\"\n)\n\n\n\n\n\n\n\n\nChanges from 2010 to 2020 (ACS 5)\n\n\n\n\n\n\n\nPopulation\n\n\nCode\nleaflet(change) %&gt;% \n    addProviderTiles(providers$CartoDB.Positron) %&gt;%  # Lighter, more professional background\n    addPolygons(\n        fillColor = ~colorPalette(pop_change_color),\n        weight = 0.4,\n        color = \"white\",\n        fillOpacity = 0.8,\n        popup = ~paste(NAME,\"&lt;br&gt;&lt;br&gt; 2020 Population: &lt;b&gt;\", comma(population_2020), \n                       \"&lt;/b&gt;&lt;br&gt; Population Change from 2010: &lt;b&gt;\", percent(pop_change, accuracy = 0.1),\n                       \"&lt;/b&gt;&lt;br&gt; Median Income Change: &lt;b&gt;\", percent(inc_change, accuracy = 0.1), \"&lt;/b&gt;\")\n    )\n\n\n\n\n\n\n\n\n\n\n\nMedian Income (inflation-adjusted)\n\n\nCode\nleaflet(change) %&gt;% \n    addProviderTiles(providers$CartoDB.Positron) %&gt;%  # Lighter, more professional background\n    addPolygons(\n        fillColor = ~colorPalette2(inc_change_color),\n        weight = 0.4,\n        color = \"white\",\n        fillOpacity = 0.8,\n        popup = ~paste(NAME,\"&lt;br&gt;&lt;br&gt; 2020 Population: &lt;b&gt;\", comma(population_2020), \n                       \"&lt;/b&gt;&lt;br&gt; 2020 Median Income: &lt;b&gt;\", dollar(median_income_2020),\n                       \"&lt;/b&gt;&lt;br&gt; Median Income Change from 2010: &lt;b&gt;\", percent(inc_change, accuracy = 0.1), \"&lt;/b&gt;\")\n    )"
  },
  {
    "objectID": "assignment_08.html#population",
    "href": "assignment_08.html#population",
    "title": "EPPS 6302: Assignment 8",
    "section": "Population",
    "text": "Population\n\n\nCode\nleaflet(change) %&gt;% \n    addProviderTiles(providers$CartoDB.Positron) %&gt;%  # Lighter, more professional background\n    addPolygons(\n        fillColor = ~colorPalette(pop_change_color),\n        weight = 0.4,\n        color = \"white\",\n        fillOpacity = 0.8,\n        popup = ~paste(NAME,\"&lt;br&gt;&lt;br&gt; 2020 Population: &lt;b&gt;\", comma(population_2020), \n                       \"&lt;/b&gt;&lt;br&gt; Population Change from 2010: &lt;b&gt;\", percent(pop_change, accuracy = 0.1),\n                       \"&lt;/b&gt;&lt;br&gt; Median Income Change: &lt;b&gt;\", percent(inc_change, accuracy = 0.1), \"&lt;/b&gt;\")\n    )"
  },
  {
    "objectID": "assignment_01.html",
    "href": "assignment_01.html",
    "title": "EPPS 6302: Assignment 1",
    "section": "",
    "text": "Touching Qualtrics was like when a League of Legends player touches grass for the first time.\n\nAh, I see. Thank you very much. I would like nothing else but to never touch Qualtrics again.\n\nIf you would like proof that I have and will continue to process Qualtrics data (without touching Qualtrics itself) feel free to examine this web report that I hand-reared."
  }
]